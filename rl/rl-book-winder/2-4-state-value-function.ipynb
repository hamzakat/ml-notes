{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Rewards with the State-Value Function\n",
    "https://rl-book.com/learn/mdp/state_value_function/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "V_{\\pi}(s) = E_{\\pi} [G|s] = E_{\\pi} [\\sum_{k=0}^{T} \\gamma^kr_k|s]\n",
    "$$\n",
    "\n",
    "where G is the return, s is the state, Î³ is the discount factor, and r is the reward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple grid environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_position = 1 # The starting position\n",
    "cliff_position = 0 # The cliff position\n",
    "end_position = 5 # The terminating state position\n",
    "reward_goal_state = 5 # Reward for reaching goal\n",
    "reward_cliff = 0 # Reward for falling off cliff\n",
    "\n",
    "def reward(current_position) -> int:\n",
    "    if current_position <= cliff_position:\n",
    "        return reward_cliff\n",
    "    if current_position >= end_position:\n",
    "        return reward_goal_state\n",
    "    return 0\n",
    "\n",
    "def is_terminating(current_position) -> bool:\n",
    "    if current_position <= cliff_position:\n",
    "        return True\n",
    "    if current_position >= end_position:\n",
    "        return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def strategy() -> int:\n",
    "    if np.random.random() >= 0.5:\n",
    "        return 1 # right\n",
    "    else:\n",
    "        return -1 # left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transitions: [1, 0]\n",
      "overall reward for this simulation iteration: 0\n",
      "updated value function result: [0. 0. 0. 0. 0. 0.]\n",
      "[0] Average reward: [0.00, 0.00, nan, nan, nan, nan]\n",
      "transitions: [1, 2, 3, 4, 3, 2, 1, 2, 3, 4, 3, 4, 5]\n",
      "overall reward for this simulation iteration: 5\n",
      "updated value function result: [ 0. 10. 15. 20. 15.  5.]\n",
      "[1] Average reward: [0.00, 3.33, 5.00, 5.00, 5.00, 5.00]\n",
      "transitions: [1, 0]\n",
      "overall reward for this simulation iteration: 0\n",
      "updated value function result: [ 0. 10. 15. 20. 15.  5.]\n",
      "[2] Average reward: [0.00, 2.50, 5.00, 5.00, 5.00, 5.00]\n",
      "transitions: [1, 0]\n",
      "overall reward for this simulation iteration: 0\n",
      "updated value function result: [ 0. 10. 15. 20. 15.  5.]\n",
      "[3] Average reward: [0.00, 2.00, 5.00, 5.00, 5.00, 5.00]\n",
      "transitions: [1, 0]\n",
      "overall reward for this simulation iteration: 0\n",
      "updated value function result: [ 0. 10. 15. 20. 15.  5.]\n",
      "[4] Average reward: [0.00, 1.67, 5.00, 5.00, 5.00, 5.00]\n",
      "transitions: [1, 0]\n",
      "overall reward for this simulation iteration: 0\n",
      "updated value function result: [ 0. 10. 15. 20. 15.  5.]\n",
      "[5] Average reward: [0.00, 1.43, 5.00, 5.00, 5.00, 5.00]\n",
      "transitions: [1, 2, 1, 0]\n",
      "overall reward for this simulation iteration: 0\n",
      "updated value function result: [ 0. 10. 15. 20. 15.  5.]\n",
      "[6] Average reward: [0.00, 1.11, 3.75, 5.00, 5.00, 5.00]\n",
      "transitions: [1, 2, 1, 0]\n",
      "overall reward for this simulation iteration: 0\n",
      "updated value function result: [ 0. 10. 15. 20. 15.  5.]\n",
      "[7] Average reward: [0.00, 0.91, 3.00, 5.00, 5.00, 5.00]\n",
      "transitions: [1, 0]\n",
      "overall reward for this simulation iteration: 0\n",
      "updated value function result: [ 0. 10. 15. 20. 15.  5.]\n",
      "[8] Average reward: [0.00, 0.83, 3.00, 5.00, 5.00, 5.00]\n",
      "transitions: [1, 0]\n",
      "overall reward for this simulation iteration: 0\n",
      "updated value function result: [ 0. 10. 15. 20. 15.  5.]\n",
      "[9] Average reward: [0.00, 0.77, 3.00, 5.00, 5.00, 5.00]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\H\\AppData\\Local\\Temp\\ipykernel_39664\\2592133396.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  expected_return = ', '.join(f'{q:.2f}' for q in value_sum / n_hits)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Global buffers to perform averaging later\n",
    "value_sum = np.zeros(end_position + 1)\n",
    "n_hits = np.zeros(end_position + 1)\n",
    "\n",
    "n_iter = 10\n",
    "for i in range(n_iter):\n",
    "    position_history = [] # A log of positions in this episode\n",
    "    current_position = starting_position # Reset\n",
    "    while True:\n",
    "        # Append position to log\n",
    "        position_history.append(current_position)\n",
    "\n",
    "        if is_terminating(current_position):\n",
    "            break\n",
    "        \n",
    "        # Update current position according to strategy\n",
    "        current_position += strategy()\n",
    "\n",
    "    # Now the episode has finished, what was the reward?\n",
    "    current_reward = reward(current_position)\n",
    "    \n",
    "\n",
    "    # Now add the reward to the buffers that allow you to calculate the average\n",
    "    for pos in position_history:\n",
    "        value_sum[pos] += current_reward\n",
    "        n_hits[pos] += 1\n",
    "    \n",
    "    \n",
    "    print(\"transitions:\", position_history)\n",
    "    print(\"overall reward for this simulation iteration:\", current_reward)\n",
    "    print(\"updated value function result:\", value_sum)\n",
    "    \n",
    "    # Now calculate the average for this episode and print\n",
    "    expected_return = ', '.join(f'{q:.2f}' for q in value_sum / n_hits)\n",
    "    print(\"[{}] Average reward: [{}]\".format(i, expected_return))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
