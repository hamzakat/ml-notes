{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will be investigating the fundamentals of reinforcement learning (RL). The first section describes the Markov decision process (MDP), which is a framework to help you design problems. The second section formulates an RL-driven solution for the MDP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. The Markov Decision Process (MDP)\n",
    "\n",
    "- MDP is used to encapsulate the real-world\n",
    "- MDP has 2 entities:\n",
    "    1. **Agent**: RL algorithm\n",
    "    2. **Environment**: real life or simulation\n",
    "\n",
    "- MDP Interface: you need representations of:\n",
    "    1. **State**: what to show to the agent. It's an observation of the environment, which contains everything outside of the agent\n",
    "    2. **Action**: suggested by the agent\n",
    "    3. **Reward**: you use it to fine tune the action choices\n",
    "-  The environmentâ€™s step function accepts an **action**, then produces a new **state** and **reward**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, namedtuple\n",
    "from enum import Enum\n",
    "from typing import Tuple, List\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "Point = namedtuple('Point', ['x', 'y'])\n",
    "class Direction(Enum):\n",
    "  NORTH = \"â¬†\"\n",
    "  EAST = \"â®•\"\n",
    "  SOUTH = \"â¬‡\"\n",
    "  WEST = \"â¬…\"\n",
    "\n",
    "  @classmethod\n",
    "  def values(self):\n",
    "    return [v for v in self]\n",
    "\n",
    "class SimpleGridWorld(object):\n",
    "    def __init__(self, width: int = 5, height: int = 5, debug: bool = False):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.debug = debug\n",
    "        self.action_space = [d for d in Direction]\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_pos = Point(x=0, y=(self.height - 1))\n",
    "        self.goal = Point(x=(self.width - 1), y=0)\n",
    "        # If debug, print state\n",
    "        if self.debug:\n",
    "            print(self)\n",
    "        return self.cur_pos, 0, False\n",
    "\n",
    "    # remember:  \n",
    "    # The environmentâ€™s step function accepts an action, then produces a new state and reward.\n",
    "    def step(self, action: Direction):\n",
    "        # Depending on the action, mutate the environment state\n",
    "        if action == Direction.NORTH:\n",
    "            self.cur_pos = Point(self.cur_pos.x, self.cur_pos.y + 1)\n",
    "        elif action == Direction.EAST:\n",
    "            self.cur_pos = Point(self.cur_pos.x + 1, self.cur_pos.y)\n",
    "        elif action == Direction.SOUTH:\n",
    "            self.cur_pos = Point(self.cur_pos.x, self.cur_pos.y - 1)\n",
    "        elif action == Direction.WEST:\n",
    "            self.cur_pos = Point(self.cur_pos.x - 1, self.cur_pos.y)\n",
    "        # Check if out of bounds\n",
    "        if self.cur_pos.x >= self.width:\n",
    "            self.cur_pos = Point(self.width - 1, self.cur_pos.y)\n",
    "        if self.cur_pos.y >= self.height:\n",
    "            self.cur_pos = Point(self.cur_pos.x, self.height - 1)\n",
    "        if self.cur_pos.x < 0:\n",
    "            self.cur_pos = Point(0, self.cur_pos.y)\n",
    "        if self.cur_pos.y < 0:\n",
    "            self.cur_pos = Point(self.cur_pos.x, 0)\n",
    "\n",
    "        # If at goal, terminate\n",
    "        is_terminal = self.cur_pos == self.goal\n",
    "\n",
    "        # Constant -1 reward to promote speed-to-goal\n",
    "        reward = -1\n",
    "\n",
    "        # If debug, print state\n",
    "        if self.debug:\n",
    "            print(self)\n",
    "\n",
    "        return self.cur_pos, reward, is_terminal\n",
    "    \n",
    "    \n",
    "    # visualisation\n",
    "    def __repr__(self):\n",
    "        res = \"\"\n",
    "        for y in reversed(range(self.height)):\n",
    "            for x in range(self.width):\n",
    "                if self.goal.x == x and self.goal.y == y:\n",
    "                    if self.cur_pos.x == x and self.cur_pos.y == y:\n",
    "                        res += \"@\"\n",
    "                    else:\n",
    "                        res += \"o\"\n",
    "                    continue\n",
    "                if self.cur_pos.x == x and self.cur_pos.y == y:\n",
    "                    res += \"x\"\n",
    "                else:\n",
    "                    res += \"_\"\n",
    "            res += \"\\n\"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "â˜ This shows a simple visualisation of the environment state.\n",
      "\n",
      "_____\n",
      "x____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "_____\n",
      "_____\n",
      "x____\n",
      "_____\n",
      "____o\n",
      "\n",
      "(Point(x=0, y=2), -1, False) â¬… This displays the state and reward from the environment ð€ð…ð“ð„ð‘ moving.\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "x____\n",
      "____o\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "x___o\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_x__o\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "__x_o\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "___xo\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____@\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Point(x=4, y=0), -1, True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = SimpleGridWorld(debug=True)\n",
    "print(\"â˜ This shows a simple visualisation of the environment state.\\n\")\n",
    "s.step(Direction.SOUTH)\n",
    "print(s.step(Direction.SOUTH), \"â¬… This displays the state and reward from the environment ð€ð…ð“ð„ð‘ moving.\\n\")\n",
    "s.step(Direction.SOUTH)\n",
    "s.step(Direction.SOUTH)\n",
    "s.step(Direction.EAST)\n",
    "s.step(Direction.EAST)\n",
    "s.step(Direction.EAST)\n",
    "s.step(Direction.EAST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. RL Solution to the MDP: Monte Carlo Methods\n",
    "\n",
    "Monte Carlo methods are used in modern RL methods to randomly sample states and judge their value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating trajectories\n",
    "The idea of MC techniques is that if we can sample the environment enough times, we can begin to build a picture of the output given any input.\n",
    "If we capture enough tragectories (a trajectory is a one full pass through the env.), then we can see which states are advantagous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a class that's capable of generating trajectories by passing in the environment as an `object`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloGeneration(object):\n",
    "    def __init__(self, env: SimpleGridWorld, max_steps: int = 1000, debug: bool = False):\n",
    "        self.env = env\n",
    "        self.max_steps = max_steps\n",
    "        self.debug = debug\n",
    "\n",
    "    def run(self) -> List:\n",
    "        buffer = []\n",
    "        n_steps = 0\n",
    "        state, _, _ = self.env.reset()\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            action = random.choice(self.env.action_space) # Random action. Try replacing this with Direction.EAST\n",
    "            next_state, reward, terminal = self.env.step(action) # Take action in environment\n",
    "            buffer.append((state, action, reward)) # Store the result\n",
    "            state = next_state # Ready for the next step\n",
    "            \n",
    "            n_steps += 1\n",
    "            if n_steps >= self.max_steps:\n",
    "                if self.debug:\n",
    "                    print(\"Terminated early due to large number of steps\")\n",
    "                terminal = True\n",
    "        return buffer # return the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "x____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "x____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "x____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "x____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "_____\n",
      "x____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "x____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "x____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "_x___\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "x____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "x____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "x____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "Terminated early due to large number of steps\n",
      "['â¬…', 'â¬†', 'â¬†', 'â¬‡', 'â¬†', 'â¬†', 'â®•', 'â¬…', 'â¬†', 'â¬†']\n",
      "total reward: -10\n"
     ]
    }
   ],
   "source": [
    "env = SimpleGridWorld(debug=True) # Instantiate the environment\n",
    "generator = MonteCarloGeneration(env=env, max_steps=10, debug=True)\n",
    "trajectory = generator.run()\n",
    "print([t[1].value for t in trajectory]) # Print chosen actions\n",
    "print(f\"total reward: {sum([t[2] for t in trajectory])}\") # Print final reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying Value\n",
    "\n",
    "We will quantofy the value isong *action-value function*: it is a measure of the value of taking a particular action, given all the experience (i.e. the previous trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make a class that runs an experiment by generating a full trajectory from start to termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloExperiment(object):\n",
    "    def __init__(self, generator: MonteCarloGeneration):\n",
    "        self.generator = generator\n",
    "        self.values = defaultdict(float)\n",
    "        self.counts = defaultdict(float)\n",
    "\n",
    "\n",
    "    def _to_key(self, state, action):\n",
    "        return (state, action)\n",
    "\n",
    "    def action_value(self, state, action) -> float:\n",
    "        key = self._to_key(state, action)\n",
    "        if self.counts[key] > 0:\n",
    "            return self.values[key] / self.counts[key]\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    # find the value of an action, on average\n",
    "    def run_episode(self) -> None:\n",
    "        trajectory = self.generator.run() # Generate a trajectory\n",
    "        episode_reward = 0\n",
    "        for i, t in enumerate(reversed(trajectory)): # Starting from the terminal state\n",
    "            state, action, reward = t\n",
    "            key = self._to_key(state, action)\n",
    "            episode_reward += reward  # Add the reward to the buffer\n",
    "            self.values[key] += episode_reward # And add this to the value of this action\n",
    "            self.counts[key] += 1 # Increment counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0:  [-20.0, -1.0, 0.0, -34.5]\n",
      "['â¬†', 'â®•', 'â¬‡', 'â¬…']\n",
      "Run 1:  [-16.5, -1.0, 0.0, -34.666666666666664]\n",
      "['â¬†', 'â®•', 'â¬‡', 'â¬…']\n",
      "Run 2:  [-77.4, -1.0, 0.0, -69.16666666666667]\n",
      "['â¬†', 'â®•', 'â¬‡', 'â¬…']\n",
      "Run 3:  [-77.4, -1.0, 0.0, -69.16666666666667]\n",
      "['â¬†', 'â®•', 'â¬‡', 'â¬…']\n"
     ]
    }
   ],
   "source": [
    "env = SimpleGridWorld(debug=False) # Instantiate the environment - set the debug to true to see the actual movemen of the agent.\n",
    "generator = MonteCarloGeneration(env=env, debug=True) # Instantiate the trajectory generator\n",
    "agent = MonteCarloExperiment(generator=generator)\n",
    "for i in range(4):\n",
    "    agent.run_episode()\n",
    "    # Point(3,0) is a step away from the goal @\n",
    "    print(f\"Run {i}: \", [agent.action_value(Point(3,0), d) for d in env.action_space])\n",
    "    print([d.value for d in env.action_space])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-77.14 | -41.38 | -28.57 | -14.00 | -26.00 | \n",
      "-82.03 | -66.35 | -39.17 | -19.25 | -13.38 | \n",
      "-64.90 | -64.25 | -35.12 | -34.00 | -34.38 | \n",
      "-64.83 | -41.71 | -36.92 | -44.75 | -36.50 | \n",
      "-82.12 | -57.67 | -43.02 | -29.83 |    @   | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def state_value_2d(env, agent):\n",
    "    res = \"\"\n",
    "    for y in reversed(range(env.height)):\n",
    "      for x in range(env.width):\n",
    "        if env.goal.x == x and env.goal.y == y:\n",
    "          res += \"   @  \"\n",
    "        else:\n",
    "          state_value = sum([agent.action_value(Point(x,y), d) for d in env.action_space]) / len(env.action_space)\n",
    "          res += f'{state_value:6.2f}'\n",
    "        res += \" | \"\n",
    "      res += \"\\n\"\n",
    "    return res\n",
    "\n",
    "print(state_value_2d(env, agent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that the value increases the closer you get to the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Optimal Policies\n",
    "The optimal policy is to choose the action that picks the highest expected return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 999\n",
      "-107.63 | -107.07 | -101.09 | -96.49 | -90.60 | \n",
      "-106.98 | -104.33 | -97.27 | -93.74 | -86.14 | \n",
      "-101.87 | -99.03 | -93.17 | -83.35 | -75.48 | \n",
      "-99.30 | -92.73 | -83.85 | -64.51 | -47.91 | \n",
      "-97.60 | -90.73 | -74.20 | -45.40 |    @   | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = SimpleGridWorld(debug=False) # Instantiate the environment - set the debug to true to see the actual movemen of the agent.\n",
    "generator = MonteCarloGeneration(env=env, debug=True) # Instantiate the trajectory generator\n",
    "agent = MonteCarloExperiment(generator=generator)\n",
    "for i in range(1000):\n",
    "    clear_output(wait=True)\n",
    "    agent.run_episode()\n",
    "    print(f\"Iteration: {i}\")\n",
    "    print(state_value_2d(env, agent), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬‡ | â®• | â®• | â®• | â¬‡ | \n",
      "â®• | â®• | â¬‡ | â¬‡ | â¬‡ | \n",
      "â®• | â®• | â®• | â¬‡ | â¬‡ | \n",
      "â®• | â®• | â®• | â¬‡ | â¬‡ | \n",
      "â®• | â®• | â®• | â®• | @ | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def argmax(a):\n",
    "    return max(range(len(a)), key=lambda x: a[x])\n",
    "\n",
    "def next_best_value_2d(env, agent):\n",
    "    res = \"\"\n",
    "    for y in reversed(range(env.height)):\n",
    "      for x in range(env.width):\n",
    "        if env.goal.x == x and env.goal.y == y:\n",
    "          res += \"@\"\n",
    "        else:\n",
    "          # Find the action that has the highest value\n",
    "          loc = argmax([agent.action_value(Point(x,y), d) for d in env.action_space])\n",
    "          res += f'{env.action_space[loc].value}'\n",
    "        res += \" | \"\n",
    "      res += \"\\n\"\n",
    "    return res\n",
    "\n",
    "print(next_best_value_2d(env, agent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running it more times, we get better policy. Let's how policy converges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 999\n",
      "-106.67 | -102.87 | -96.26 | -95.36 | -95.13 | \n",
      "-103.15 | -100.87 | -94.50 | -91.39 | -86.13 | \n",
      "-103.69 | -97.02 | -87.86 | -80.85 | -73.98 | \n",
      "-103.23 | -94.83 | -82.42 | -66.86 | -50.67 | \n",
      "-99.92 | -92.62 | -76.92 | -48.59 |    @   | \n",
      "\n",
      "â®• | â¬‡ | â¬† | â¬† | â¬‡ | \n",
      "â¬… | â®• | â¬‡ | â¬‡ | â¬‡ | \n",
      "â®• | â®• | â¬‡ | â¬‡ | â¬‡ | \n",
      "â®• | â®• | â®• | â¬‡ | â¬‡ | \n",
      "â®• | â®• | â®• | â®• | @ | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env = SimpleGridWorld() # Instantiate the environment\n",
    "generator = MonteCarloGeneration(env=env) # Instantiate the trajectory generator\n",
    "agent = MonteCarloExperiment(generator=generator)\n",
    "for i in range(1000):\n",
    "    clear_output(wait=True)\n",
    "    agent.run_episode()\n",
    "    print(f\"Iteration: {i}\")\n",
    "    print(state_value_2d(env, agent))\n",
    "    print(next_best_value_2d(env, agent), flush=True)\n",
    "    time.sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the agent is still totally random at this point. The agent started in the top left, so on average, it takes a lot of stumbling to find the goal. Therefore, for the points at the top left, furthest away from the goal, the agent will probably take many more random steps before it reachest the goal. That's why we get incorrect arrows at the top (especcially in the very first iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
