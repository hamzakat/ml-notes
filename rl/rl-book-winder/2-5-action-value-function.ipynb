{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Rewards with the Action Value Function\n",
    "\n",
    "https://rl-book.com/learn/mdp/action_value_function/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functions is same as state-value function, but adds the action to the consideration. In this case, action is a second dimension of the value/reward prediction problem\n",
    "\n",
    "$$\n",
    "Q_{\\pi}(s, a) = E_{\\pi} [G|s, a] = E_{\\pi} [\\sum_{k=0}^{T} \\gamma^kr_k|s, a]\n",
    "$$\n",
    "\n",
    "where G is the return, s is the state, a is the action choise, Î³ is the discount factor, and r is the reward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple grid environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_position = 1 # The starting position\n",
    "cliff_position = 0 # The cliff position\n",
    "end_position = 5 # The terminating state position\n",
    "reward_goal_state = 5 # Reward for reaching goal\n",
    "reward_cliff = 0 # Reward for falling off cliff\n",
    "\n",
    "def reward(current_position) -> int:\n",
    "    if current_position <= cliff_position:\n",
    "        return reward_cliff\n",
    "    if current_position >= end_position:\n",
    "        return reward_goal_state\n",
    "    return 0\n",
    "\n",
    "def is_terminating(current_position) -> bool:\n",
    "    if current_position <= cliff_position:\n",
    "        return True\n",
    "    if current_position >= end_position:\n",
    "        return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def strategy() -> int:\n",
    "    if np.random.random() >= 0.5:\n",
    "        return 1 # right\n",
    "    else:\n",
    "        return -1 # left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transitions: (1, -1)\n",
      "[0] Average reward: [nan, 5.00, 5.00, 5.00, 5.00, 5.00 ; nan, nan, nan, nan, nan, nan]\n",
      "transitions: (1, -1)\n",
      "[1] Average reward: [0.00, 3.33, 5.00, 5.00, 5.00, 5.00 ; nan, nan, nan, nan, nan, nan]\n",
      "transitions: (1, -1)\n",
      "[2] Average reward: [0.00, 2.50, 5.00, 5.00, 5.00, 5.00 ; nan, nan, nan, nan, nan, nan]\n",
      "transitions: (1, 1)\n",
      "[3] Average reward: [0.00, 2.50, 5.00, 5.00, 5.00, 5.00 ; 0.00, 0.00, nan, nan, nan, nan]\n",
      "transitions: (1, -1)\n",
      "[4] Average reward: [0.00, 1.67, 3.75, 5.00, 5.00, 5.00 ; 0.00, 0.00, nan, nan, nan, nan]\n",
      "transitions: (1, -1)\n",
      "[5] Average reward: [0.00, 1.43, 3.75, 5.00, 5.00, 5.00 ; 0.00, 0.00, nan, nan, nan, nan]\n",
      "transitions: (1, 1)\n",
      "[6] Average reward: [0.00, 1.43, 3.75, 5.00, 5.00, 5.00 ; 0.00, 0.00, nan, nan, nan, nan]\n",
      "transitions: (1, 1)\n",
      "[7] Average reward: [0.00, 1.43, 3.75, 5.00, 5.00, 5.00 ; 0.00, 0.00, 0.00, nan, nan, nan]\n",
      "transitions: (1, 1)\n",
      "[8] Average reward: [0.00, 1.43, 3.75, 5.00, 5.00, 5.00 ; 0.00, 0.00, 0.00, 0.00, nan, nan]\n",
      "transitions: (1, -1)\n",
      "[9] Average reward: [0.00, 1.25, 3.75, 5.00, 5.00, 5.00 ; 0.00, 0.00, 0.00, 0.00, nan, nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\H\\AppData\\Local\\Temp\\ipykernel_4540\\2391626809.py:41: RuntimeWarning: invalid value encountered in divide\n",
      "  f'{q:.2f}' for q in value_sum[:, 0] / n_hits[:, 0])\n",
      "C:\\Users\\H\\AppData\\Local\\Temp\\ipykernel_4540\\2391626809.py:43: RuntimeWarning: invalid value encountered in divide\n",
      "  f'{q:.2f}' for q in value_sum[:, 1] / n_hits[:, 1])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Global buffers to perform averaging later\n",
    "# Second dimension is the actions\n",
    "value_sum = np.zeros((end_position + 1, 2))\n",
    "n_hits = np.zeros((end_position + 1, 2))\n",
    "\n",
    "# A helper function to map the actions to valid buffer indices\n",
    "def action_value_mapping(x): return 0 if x == -1 else 1\n",
    "\n",
    "n_iter = 10\n",
    "for i in range(n_iter):\n",
    "    position_history = [] # A log of positions in this episode\n",
    "    current_position = starting_position # Reset\n",
    "    current_action = strategy()\n",
    "    while True:\n",
    "        # Append position to log\n",
    "        position_history.append((current_position, current_action))\n",
    "\n",
    "        if is_terminating(current_position):\n",
    "            break\n",
    "        \n",
    "        # Update current position according to strategy\n",
    "        current_position += strategy()\n",
    "\n",
    "    # Now the episode has finished, what was the reward?\n",
    "    current_reward = reward(current_position)\n",
    "    \n",
    "\n",
    "    # Now add the reward to the buffers that allow you to calculate the average\n",
    "    for pos, act in position_history:\n",
    "        value_sum[pos, action_value_mapping(act)] += current_reward\n",
    "        n_hits[pos, action_value_mapping(act)] += 1\n",
    "    \n",
    "    \n",
    "\n",
    "    # Now calculate the average for this episode and print\n",
    "    expect_return_0 = ', '.join(\n",
    "        f'{q:.2f}' for q in value_sum[:, 0] / n_hits[:, 0])\n",
    "    expect_return_1 = ', '.join(\n",
    "        f'{q:.2f}' for q in value_sum[:, 1] / n_hits[:, 1])\n",
    "    \n",
    "    print(\"transitions:\", position_history[0])\n",
    "    print(\"[{}] Average reward: [{} ; {}]\".format(\n",
    "        i, expect_return_0, expect_return_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
